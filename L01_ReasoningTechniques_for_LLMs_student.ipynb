{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Reasoning Techniques for Large Language Models\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Dr Chao Shu (chao.shu@qmul.ac.uk)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from collections import Counter\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Updated imports for LCEL\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, FewShotPromptTemplate\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction to LLM Reasoning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Large Language Models (LLMs) like GPT-4, LLaMA, and others have demonstrated impressive capabilities across various tasks. However, their ability to reason through complex problems isn't inherently straightforward.\n",
    "\n",
    "**What is reasoning in LLMs?**\n",
    "- The ability to process information logically\n",
    "- Breaking down complex problems into steps\n",
    "- Making inferences based on provided context\n",
    "- Arriving at conclusions through structured thinking\n",
    "\n",
    "**Why is reasoning important?**\n",
    "- Enables solving complex problems that require multi-step thinking\n",
    "- Improves transparency of model decision-making\n",
    "- Enhances reliability and reduces hallucinations\n",
    "- Makes LLMs more useful for specialized domains (mathematics, programming, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### üßë‚Äçüè´ Demo: Responses with and without Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's see the difference in responses from a non-reasoning (instruction) model and a reasoning model. Both models are small-scale open-weight model running locally.\n",
    "\n",
    "ü§ñ Example Prompts:\n",
    "\n",
    "1. \"Answer the questions briefly and directly with just a few words. \n",
    "\n",
    "Richard lives in an apartment building with 15 floors. Each floor contains 8 units, and 3/4 of the building is occupied. What's the total number of unoccupied units In the building?\" (*from [GSM8K dataset](https://huggingface.co/datasets/openai/gsm8k)*)\n",
    "\n",
    "2. \"how many 'r's in 'strawberry'?\"\n",
    "\n",
    "3. \"9.7 and 9.11, which number is bigger?\"\n",
    "\n",
    "üß† Critical Thinking: What caused the differences? How the reasoning capability is developed in reasoning models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's use LangChain to demo the Q&A in a programmatical way using simplest codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Non-reasoning model response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='21 - 3/4 * 15 = 9', additional_kwargs={}, response_metadata={'model': 'qwen2:1.5b', 'created_at': '2025-03-25T09:04:49.9234888Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4924381300, 'load_duration': 2985276200, 'prompt_eval_count': 64, 'prompt_eval_duration': 1412249100, 'eval_count': 15, 'eval_duration': 524833100, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-ab70aa5c-4299-4be4-8334-d3355b477d4b-0', usage_metadata={'input_tokens': 64, 'output_tokens': 15, 'total_tokens': 79})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = \"\"\"Answer the questions briefly and directly with just a few words. \n",
    "Richard lives in an apartment building with 15 floors. Each floor contains 8 units, and 3/4 of the building is occupied. What's the total number of unoccupied units In the building?\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the Ollama LLM\n",
    "llm_qwen = ChatOllama(model=\"qwen2:1.5b\", temperature=0.7)\n",
    "\n",
    "output = llm_qwen.invoke(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Reasoning model response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I have this problem here about Richard living in an apartment building. Let me try to figure out how to solve it step by step. First, the question says that Richard lives on a building with 15 floors. Each floor has 8 units. Out of all these units, 3/4 of the building is occupied. I need to find the total number of unoccupied units.\n",
      "\n",
      "Hmm, let me break this down. The building has 15 floors, each with 8 units. So first, maybe I should calculate the total number of units in the building. That seems straightforward‚Äîjust multiply the number of floors by the number of units per floor. So that would be 15 multiplied by 8.\n",
      "\n",
      "Let me write that out: 15 * 8 = ?\n",
      "\n",
      "Hmm, 15 times 8... 10*8 is 80, and 5*8 is 40, so adding those together gives 120. So the total number of units in the building is 120.\n",
      "\n",
      "Now, out of these 120 units, 3/4 are occupied. To find out how many are unoccupied, I think I need to calculate what's left after accounting for the occupied ones. Since 3/4 are occupied, that means 1 - 3/4 = 1/4 are unoccupied.\n",
      "\n",
      "So if there are 120 units in total and 1/4 of them are unoccupied, then the number of unoccupied units would be (1/4)*120. Let me calculate that: 120 divided by 4 is 30. So, there should be 30 unoccupied units.\n",
      "\n",
      "Wait a second, let me make sure I didn't miss anything. The building has 15 floors with 8 units each, so total units are indeed 120. If 3/4 of that is occupied, then the remaining 1/4 must be unoccupied. So yes, 30 makes sense.\n",
      "\n",
      "Is there any chance I could have made a mistake in calculating the total units? Let me double-check: 15 times 8. 10*8=80, 5*8=40, 80+40=120. Yep, that's correct.\n",
      "\n",
      "And for the unoccupied part: 3/4 occupied means 1/4 is not. So 120*(1/4) = 30. That seems right. I don't think there are any other factors here‚Äîno mention of broken units or anything else. So, yeah, I'm confident that the number of unoccupied units is 30.\n",
      "</think>\n",
      "\n",
      "The total number of unoccupied units in the building is 30.\n",
      "\n",
      "Step-by-step explanation:\n",
      "1. Calculate the total number of units: 15 floors * 8 units/floor = 120 units.\n",
      "2. Determine the fraction of occupied units: 3/4 are occupied, so 1 - 3/4 = 1/4 are unoccupied.\n",
      "3. Find the number of unoccupied units: (1/4) * 120 = 30.\n",
      "\n",
      "Answer: \\boxed{30}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Ollama LLM\n",
    "llm_deepseek = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.7)\n",
    "\n",
    "output = llm_deepseek.invoke(input)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, let's demonstrate how to use LangChain Expression Language (LCEL) to build a flexible pipeline for the Q&A. This time let's try Llama 3.2 3B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='To find the total number of unoccupied units, we need to first calculate the total number of units in the building.\\n\\nThe total number of units = Number of floors x Number of units per floor\\n= 15 x 8\\n= 120 units\\n\\nSince 3/4 of the building is occupied, 1/4 of the building remains unoccupied. \\n\\nTo find the number of unoccupied units:\\n= Total units x Unoccupied fraction\\n= 120 x 1/4\\n= 30 units', additional_kwargs={}, response_metadata={'model': 'llama3.2:3b', 'created_at': '2025-03-25T09:15:33.7981876Z', 'done': True, 'done_reason': 'stop', 'total_duration': 13676134400, 'load_duration': 3988633900, 'prompt_eval_count': 80, 'prompt_eval_duration': 2736203100, 'eval_count': 106, 'eval_duration': 6918341300, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-a8be4049-a89c-45f6-a430-c7cb57a7701c-0', usage_metadata={'input_tokens': 80, 'output_tokens': 106, 'total_tokens': 186})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a prompt template from the template string\n",
    "template = \"Answer the questions briefly and directly with just a few words. \\n\\n{question}\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "question = \"Richard lives in an apartment building with 15 floors. Each floor contains 8 units, and 3/4 of the building is occupied. What's the total number of unoccupied units In the building?\"\n",
    "\n",
    "# Initialize the Ollama LLM\n",
    "llm_llama32 = ChatOllama(model=\"llama3.2:3b\", temperature=0.7)\n",
    "\n",
    "# Create the LLM chain\n",
    "llm_chain = prompt | llm_llama32 \n",
    "\n",
    "response = llm_chain.invoke({\"question\": question})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Alternatively, we can simply extract the text part in the response by adding one more stage to the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "model \"llama3.2:3b\" not found, try pulling it first (status code: 404)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create the LLM chain with the text content extractor\u001b[39;00m\n\u001b[32m      2\u001b[39m llm_chain = prompt | llm_llama32 | StrOutputParser()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m response = \u001b[43mllm_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\ReasoningAndAgents\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3025\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3023\u001b[39m                 \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m   3024\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3025\u001b[39m                 \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3026\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3027\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\ReasoningAndAgents\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:307\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    297\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    298\u001b[39m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[32m   (...)\u001b[39m\u001b[32m    302\u001b[39m     **kwargs: Any,\n\u001b[32m    303\u001b[39m ) -> BaseMessage:\n\u001b[32m    304\u001b[39m     config = ensure_config(config)\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    306\u001b[39m         ChatGeneration,\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    317\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\ReasoningAndAgents\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:843\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    835\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    836\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    837\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[32m   (...)\u001b[39m\u001b[32m    840\u001b[39m     **kwargs: Any,\n\u001b[32m    841\u001b[39m ) -> LLMResult:\n\u001b[32m    842\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\ReasoningAndAgents\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:683\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    680\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[32m    681\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    682\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m683\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    689\u001b[39m         )\n\u001b[32m    690\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    691\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\ReasoningAndAgents\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:908\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    906\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    907\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m908\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    911\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    912\u001b[39m         result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\ReasoningAndAgents\\Lib\\site-packages\\langchain_ollama\\chat_models.py:705\u001b[39m, in \u001b[36mChatOllama._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m    699\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    700\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    703\u001b[39m     **kwargs: Any,\n\u001b[32m    704\u001b[39m ) -> ChatResult:\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m     final_chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     generation_info = final_chunk.generation_info\n\u001b[32m    709\u001b[39m     chat_generation = ChatGeneration(\n\u001b[32m    710\u001b[39m         message=AIMessage(\n\u001b[32m    711\u001b[39m             content=final_chunk.text,\n\u001b[32m   (...)\u001b[39m\u001b[32m    716\u001b[39m         generation_info=generation_info,\n\u001b[32m    717\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\ReasoningAndAgents\\Lib\\site-packages\\langchain_ollama\\chat_models.py:642\u001b[39m, in \u001b[36mChatOllama._chat_stream_with_aggregation\u001b[39m\u001b[34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_stream_with_aggregation\u001b[39m(\n\u001b[32m    634\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    635\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    639\u001b[39m     **kwargs: Any,\n\u001b[32m    640\u001b[39m ) -> ChatGenerationChunk:\n\u001b[32m    641\u001b[39m     final_chunk = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m642\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterate_over_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\ReasoningAndAgents\\Lib\\site-packages\\langchain_ollama\\chat_models.py:727\u001b[39m, in \u001b[36mChatOllama._iterate_over_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_iterate_over_stream\u001b[39m(\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    722\u001b[39m     messages: List[BaseMessage],\n\u001b[32m    723\u001b[39m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    724\u001b[39m     **kwargs: Any,\n\u001b[32m    725\u001b[39m ) -> Iterator[ChatGenerationChunk]:\n\u001b[32m    726\u001b[39m     is_thinking = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m727\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mChatGenerationChunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mAIMessageChunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   (...)\u001b[39m\u001b[32m    744\u001b[39m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    745\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\ReasoningAndAgents\\Lib\\site-packages\\langchain_ollama\\chat_models.py:629\u001b[39m, in \u001b[36mChatOllama._create_chat_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    626\u001b[39m chat_params = \u001b[38;5;28mself\u001b[39m._chat_params(messages, stop, **kwargs)\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    631\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\ReasoningAndAgents\\Lib\\site-packages\\ollama\\_client.py:168\u001b[39m, in \u001b[36mClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    167\u001b[39m   e.response.read()\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r.iter_lines():\n\u001b[32m    171\u001b[39m   part = json.loads(line)\n",
      "\u001b[31mResponseError\u001b[39m: model \"llama3.2:3b\" not found, try pulling it first (status code: 404)"
     ]
    }
   ],
   "source": [
    "# Create the LLM chain with the text content extractor\n",
    "llm_chain = prompt | llm_llama32 | StrOutputParser()\n",
    "\n",
    "response = llm_chain.invoke({\"question\": question})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If you want to display the response text in Markdown format in a pretty way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To find the total number of unoccupied units, we need to know how many units are occupied and subtract that from the total.\n",
       "\n",
       "First, find the total number of units in the building: 15 floors x 8 units per floor = 120 units.\n",
       "\n",
       "Next, calculate the number of occupied units: (3/4) x 120 units = 90 units.\n",
       "\n",
       "Finally, subtract the occupied units from the total to get the unoccupied units: 120 - 90 = 30 units.\n",
       "\n",
       "So, there are 30 unoccupied units in the building."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, you know the basics to build AI apps using LangChain. üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spectrum of Reasoning Capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Modern LLMs demonstrate a range of reasoning abilities across different domains:\n",
    "\n",
    "- **Arithmetic reasoning**: Solving mathematical problems with calculations\n",
    "- **Math Problem-Solving**: Solving challenging math problems\n",
    "- **Scientific Reasoning**: Making inferences based on scientific domain knowledge\n",
    "- **Commonsense reasoning**: Making inferences based on everyday knowledge\n",
    "- **Lognical Reasoning**: Drawing valid conclusions from premises\n",
    "- **Visual Reasoning**: Making inferences based on everyday knowledge\n",
    "- **etc.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Arithmetic Reasoning: Solving mathematical problems with calculations\n",
    "\n",
    "- Sample: \"Q: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? A: 72\"\n",
    "- Example Dataset: [GSM8K (Grade School Math 8K)](https://huggingface.co/datasets/openai/gsm8k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2. Math Problem-Solving: Solving challenging math problems\n",
    "- Sample: \"Q: If $f(x) = \\frac{3x-2}{x-2}$, what is the value of $f(-2) +f(-1)+f(0)$? Express your answer as a common fraction.  A: \\frac{14}{3}\"\n",
    "- Example Dataset: [MATH 500](https://huggingface.co/datasets/HuggingFaceH4/MATH-500)\n",
    "- Sample: \"Q: There exist real numbers $x$ and $y$, both greater than 1, such that $\\log_x\\left(y^x\\right)=\\log_y\\left(x^{4y}\\right)=10$. Find $xy$.  A: 25\"\n",
    "- Example Dataset: [AIME (American Invitational Mathematics Examination) 2024](https://huggingface.co/datasets/Maxwell-Jia/AIME_2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3. Scientific Reasoning: Making inferences based on scientific domain knowledge\n",
    "- Description: GPQA (Graduate-Level Google-Proof Q&A) is a multiple-choice, Q&A dataset of very hard questions written and validated by experts in biology, physics, and chemistry. When attempting questions out of their own domain (e.g., a physicist answers a chemistry question), these experts get only 34% accuracy, despite spending >30m with full access to Google.\n",
    "- Example Dataset: [GPQA](https://huggingface.co/datasets/Idavidrein/gpqa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "4. Commonsense Reasoning: Making inferences based on everyday knowledge\n",
    "- Sample: \"Q: The fox walked from the city into the forest, what was it looking for? Answer Choices: (a) pretty flowers (b) hen house (c) natural habitat (d) storybook  A: (b)\"\n",
    "- Example Dataset: [CommonsenseQA](https://www.tau-nlp.sites.tau.ac.il/commonsenseqa), [HellaSwag](https://github.com/rowanz/hellaswag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "5. Logical Reasoning: Drawing valid conclusions from premises\n",
    "- Sample: \n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"imgs/L02_HotpotQA_Sample.png\" alt=\"HotpotQA Sample\" style=\"width:50%; height:auto;\">\n",
    "</div>\n",
    "\n",
    "- Example Dataset: [HotpotQA](https://hotpotqa.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "6. Visual Reasoning: Questions require an understanding of vision, language and commonsense knowledge to answer.\n",
    "- Sample: <img src=\"imgs/L02_VQA_BigBang.png\" alt=\"VQA Sample\" style=\"width:50%; height:auto;\">\n",
    "\n",
    "   - \"Q: What time of day is it?  A: Night\"\n",
    "- Example Dataset: [VQA v2 (Visual Question Answering)](https://visualqa.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SOTA Reasoning Models and Their Approaches to Enhance Reasoning Capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recent breakthroughs in artificial intelligence have led to the development of several language models with unprecedented reasoning capabilities, showcasing how innovative training methodologies and architectural choices can significantly improve a model‚Äôs ability to tackle complex problems.\n",
    "\n",
    "Reasoning models, such as OpenAI o1/o3 series and DeepSeek R1, are designed to tackle complex problem-solving tasks, particularly in domains like science, coding, and mathematics. These models go beyond traditional language generation by incorporating structured reasoning processes, where the model breaks down problems into sequential steps before arriving at a final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Below is a list of several prominent ‚Äúreasoning‚Äù‚Äêfocused large language models (LLMs). Note that many of these models emerged during late 2024 into early 2025 as part of an industry‚Äêwide push toward enhanced reasoning capabilities.\n",
    "\n",
    "| **Model**                      | **Release Date**         | **Company/Organisation**          |\n",
    "|--------------------------------|--------------------------|-----------------------------------|\n",
    "| o1‚Äëpreview                     | September 2024           | OpenAI                            |\n",
    "| o1                             | December 2024            | OpenAI                            |\n",
    "| o3‚Äëmini                        | January 2025             | OpenAI                            |\n",
    "| o3‚Äëmini‚Äëhigh                   | January 2025             | OpenAI                            |\n",
    "| Claude 3.7                     | February 2025            | Anthropic                         |\n",
    "| DeepSeek‚ÄëR1                    | January 2025             | DeepSeek     |\n",
    "| Doubao‚Äë1.5‚Äëpro                 | January 2025             | ByteDance          |\n",
    "| Kimi k1.5                      | January 2025*            | Moonshot AI                       |\n",
    "| QwQ‚Äë32B‚ÄëPreview                | December 2024*           | Alibaba Cloud                     |\n",
    "| Grok 3                         | February 2025*           | xAI          |\n",
    "| Gemini 2.0 Flash Thinking Exp. | February 2025*           | Google DeepMind                   |\n",
    "\n",
    "\\* Approximate dates based on available reports and media coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> üí¨ **Discussion:** \n",
    "> \n",
    "> What are the core techniques used to enhance the reasoning capability in the state-of-the-art reasoning models, i.e., OpenAI o1/o3, DeepSeek R1? Extract the core techniques from AI summary.\n",
    ">\n",
    "> ü§ñ Reference prompt: *\"Summarise the core techniques used to enhance the reasoning capability in the state-of-the-art reasoning models, i.e., OpenAI o1/o3, DeepSeek R1, respectively\"*\n",
    ">\n",
    "> üß† Critical Thinking: Is the information verifiable and accurate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### OpenAI o1/o3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Put the GenAI answer and your notes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### DeepSeek R1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Put the GenAI answer and your notes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, you know the techniques behind the SOTA reasoning models üöÄ. Let's delve deeper into the details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prerequisite: A Brief Introduction to Prompt Engineering\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Prompt engineering is the practice of crafting effective inputs to guide AI language models toward producing desired outputs. It involves strategically designing questions, instructions, and context to elicit accurate, relevant, and useful responses.\n",
    "\n",
    "Key aspects include:\n",
    "\n",
    "- Structuring prompts with clear instructions and constraints\n",
    "- Using specific formatting techniques\n",
    "- Providing examples to demonstrate the expected response format\n",
    "- Breaking complex tasks into sequential steps\n",
    "- Including relevant context to improve understanding\n",
    "- Setting appropriate tone, style, and level of detail\n",
    "\n",
    "Effective prompt engineering can significantly enhance AI performance across various applications, from content creation and data analysis to problem-solving and creative work. As AI systems evolve, prompt engineering continues to develop as both an art and a science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Zero-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Zero-shot prompting refers to asking an LLM to perform a task without providing specific examples of that task. The model relies solely on its pre-training knowledge.\n",
    "\n",
    "**Key characteristics:**\n",
    "- No examples provided in the prompt\n",
    "- Requires clear instructions\n",
    "- Performance varies greatly with prompt phrasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example:**\n",
    "\n",
    "Prompt:\n",
    "```Text\n",
    "What is the capital of France?\n",
    "```\n",
    "\n",
    "Output:\n",
    "```Text\n",
    "Paris\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Few-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Few-shot prompting involves providing the model with a few examples of the task before asking it to perform a similar task. This helps the model understand the expected format and reasoning style.\n",
    "\n",
    "**Key characteristics:**\n",
    "- Includes examples within the prompt\n",
    "- Helps align model output to desired format\n",
    "- Can improve performance on complex tasks\n",
    "- Examples should be representative and diverse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "**Example:** ([Brown et al. 2020](https://arxiv.org/abs/2005.14165))\n",
    "\n",
    "Prompt:\n",
    "```Text\n",
    "A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses\n",
    "the word whatpu is:\n",
    "We were traveling in Africa and we saw these very cute whatpus\n",
    "\n",
    "To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:\n",
    "```\n",
    "\n",
    "Output:\n",
    "```Text\n",
    "One day when I was playing tag with my little sister, she got really excited and she\n",
    "started doing these crazy farduddles.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Chain-of-Thought (CoT) <a id=\"cot\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Chain-of-Thought (CoT) was introduced by [Wei et al. (2022)](https://arxiv.org/abs/2201.11903) as a prompting technique to enhance the reasoning capabilities of Large Language Models (LLMs), especially in multi-step reasoning tasks.\n",
    "\n",
    "In contrast to the standard prompting, where models are asked to directly produce the final answer, 'Chain of Thought Prompting' encourages LLMs to break down complex problems into intermediate reasoning steps before arriving at the final answer. By doing this, the model-generated 'chain of thought' can mimic an intuitive human thought process when working through multi-step problems.\n",
    "\n",
    "**Key benefits:**\n",
    "- Significantly improves performance on reasoning tasks\n",
    "- Provides transparency into the model's reasoning process\n",
    "- Reduces hallucination by making each step explicit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Few-Shot CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![CoT Prompting](imgs/L02_CoT_Prompt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Examples of CoT in different reasoning tasks [(Wei et al., 2022)](https://arxiv.org/abs/2201.11903):\n",
    "\n",
    "![CoT Example Tasks](imgs/L02_CoT_ExampleTasks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Zero-Shot CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[Kojima et al. (2022)](https://arxiv.org/abs/2205.11916) introduce a simplified approach by appending the words \"Let's think step by step.\" to the end of a question. This simple prompt helps the LLM to generate a chain of thought that answers the question, from which the LLM can then extract a more accurate answer.\n",
    "\n",
    "![Zero-Shot CoT](imgs/L02_ZeroShotCot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### üßë‚Äçüè´ Demo: Reasoning by CoT Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Standard Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's start with a standard prompt as the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define the question that will be reused for different prompts\n",
    "question = \"Four friends ordered four pizzas for a total of 64 dollars. If two of the pizzas cost 30 dollars, how much did each of the other two pizzas cost if they cost the same amount?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnswer the questions briefly and directly with just a few words. \\nQuestion: \\n\\nFour friends ordered four pizzas for a total of 64 dollars. If two of the pizzas cost 30 dollars, how much did each of the other two pizzas cost if they cost the same amount?\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Create a prompt template for standard prompts\n",
    "prompt_std = PromptTemplate(input_variables=[\"question\"], \n",
    "                            template=\"\"\"\n",
    "Answer the questions briefly and directly with just a few words. \n",
    "Question: \\n\\n{question}\n",
    "\"\"\")\n",
    "\n",
    "prompt_std.invoke({\"question\": question}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Initialise a chat model using the Ollama LLM\n",
    "llm = ChatOllama(model=\"qwen2:1.5b\", temperature=0.7)\n",
    "\n",
    "# TODO: Create the LLM chain with the standard prompt template\n",
    "chain_std = prompt_std | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'25 dollars\\n\\nThe answer is: 25'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Run the chain with the question to get the string output\n",
    "response = chain_std.invoke({\"question\": question})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "25 dollars\n",
       "\n",
       "The answer is: 25"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Zero-shot CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Zero-shot Chain-of-Thought involves explicitly asking the model to reason **step by step**, without providing examples of step-by-step reasoning. This is often triggered by adding phrases like `Let's think step by step` to prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this demo, let's try to use `ChatPromptTemplate` to create the prompt, so that you can use a system prompt to define the behaviour of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: Four friends ordered four pizzas for a total of 64 dollars. If two of the pizzas cost 30 dollars, how much did each of the other two pizzas cost if they cost the same amount?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Create a zero-shot CoT prompt\n",
    "prompt_zs_cot = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the following question. Think step by step.\"),\n",
    "    (\"human\", \"Question: {question}\")]\n",
    ")\n",
    "\n",
    "prompt_zs_cot.invoke({\"question\": question}).messages[1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Initialise a chat model using the Ollama LLM\n",
    "llm = ChatOllama(model=\"qwen2:1.5b\", temperature=0.7)\n",
    "\n",
    "# TODO: Create the LLM chain with the zero-shot prompt template\n",
    "chain_zs_cot = prompt_zs_cot | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To find out how much each of the other two pizzas cost if they cost the same amount, we first need to determine the cost of one pizza.\\n\\nWe know that two pizzas cost $30. Therefore, the total cost for four pizzas is:\\n\\n\\\\[2 \\\\text{ pizzas} = 30 \\\\text{ dollars}\\\\]\\n\\nTo find out how much each of the other two pizzas costs, we divide the total cost by the number of additional pizzas (two):\\n\\n\\\\[ \\\\frac{64 \\\\text{ dollars}}{4 - 2\\\\text{ pizzas}} = \\\\frac{64}{2} = 32 \\\\text{ dollars}\\\\]\\n\\nTherefore, each of the other two pizzas cost $16.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Run the chain with the question to get the string output\n",
    "response = chain_zs_cot.invoke({\"question\": question})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To find out how much each of the other two pizzas cost if they cost the same amount, we first need to determine the cost of one pizza.\n",
       "\n",
       "We know that two pizzas cost $30. Therefore, the total cost for four pizzas is:\n",
       "\n",
       "\\[2 \\text{ pizzas} = 30 \\text{ dollars}\\]\n",
       "\n",
       "To find out how much each of the other two pizzas costs, we divide the total cost by the number of additional pizzas (two):\n",
       "\n",
       "\\[ \\frac{64 \\text{ dollars}}{4 - 2\\text{ pizzas}} = \\frac{64}{2} = 32 \\text{ dollars}\\]\n",
       "\n",
       "Therefore, each of the other two pizzas cost $16."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Few-shot CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Few-shot Chain-of-Thought combines the benefits of few-shot prompting and Chain-of-Thought reasoning. By providing examples that demonstrate step-by-step reasoning, which helps the model understand how to break down problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's firstly define the example Q&A that can be integrated in the few-shot prompt template. (6 shots selected from the Appendix G in [(Wei et al., 2022)](https://arxiv.org/abs/2201.11903))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "example_qa = \"\"\"\n",
    "Question: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "Answer: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
    "\n",
    "Question: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "Answer: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
    "\n",
    "Question: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
    "Answer: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The answer is 39.\n",
    "\n",
    "Question: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\n",
    "Answer: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. The answer is 8.\n",
    "\n",
    "Question: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\n",
    "Answer: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer is 9.\n",
    " \n",
    "Question: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
    "Answer: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. The answer is 8.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='A few examples will be provided for you to follow and understand how to think before answering the question.', additional_kwargs={}, response_metadata={}), HumanMessage(content='\"\\n\\nQuestion: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nAnswer: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\\n\\nQuestion: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nAnswer: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\\n\\nQuestion: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\\nAnswer: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The answer is 39.\\n\\nQuestion: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\\nAnswer: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. The answer is 8.\\n\\nQuestion: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\\nAnswer: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer is 9.\\n\\nQuestion: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\\nAnswer: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. The answer is 8.\\n\\n\\nNow, answer the following question\\nQuestion: Four friends ordered four pizzas for a total of 64 dollars. If two of the pizzas cost 30 dollars, how much did each of the other two pizzas cost if they cost the same amount?\\n     ', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Create a few-shot CoT prompt\n",
    "prompt_fs_cot = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"A few examples will be provided for you to follow and understand how to think before answering the question.\"),\n",
    "    (\"human\", \"\"\"\"\n",
    "{example_qa}\n",
    "     \n",
    "Now, answer the following question\n",
    "Question: {question}\n",
    "     \"\"\"),\n",
    "])\n",
    "\n",
    "prompt_fs_cot.invoke({\"example_qa\": example_qa, \"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in d:\\anaconda3\\envs\\reasoningandagents\\lib\\site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda3\\envs\\reasoningandagents\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda3\\envs\\reasoningandagents\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.5 MB 985.5 kB/s eta 0:00:12\n",
      "   --- ------------------------------------ 1.0/11.5 MB 1.2 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 1.3/11.5 MB 1.3 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 1.8/11.5 MB 1.4 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 2.1/11.5 MB 1.5 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 2.6/11.5 MB 1.6 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 3.1/11.5 MB 1.7 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 3.1/11.5 MB 1.7 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 3.9/11.5 MB 1.7 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 4.2/11.5 MB 1.7 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 4.7/11.5 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 5.2/11.5 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 5.5/11.5 MB 1.8 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 6.0/11.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 6.6/11.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 7.1/11.5 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 7.3/11.5 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 7.6/11.5 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 8.4/11.5 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 8.7/11.5 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 8.9/11.5 MB 1.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.4/11.5 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 9.7/11.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.2/11.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.5 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.0/11.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 1.8 MB/s eta 0:00:00\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "examples = pd.read_csv(\"fs.csv\")\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate.from_template(\"Question: {Question}\\nAnswer: {Answer}\")\n",
    "\n",
    "prompt_template_fs = FewShotPromptTemplate(\n",
    "    examples=examples, \n",
    "    example_prompt=example_prompt, \n",
    "    suffix=\"\"\"\n",
    "Now, answer the following question\n",
    "Question: {input}\n",
    "    \"\"\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "prompt_template_fs.invoke({\"input\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\"\n",
       "\n",
       "Question: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
       "Answer: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
       "\n",
       "Question: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
       "Answer: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
       "\n",
       "Question: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
       "Answer: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The answer is 39.\n",
       "\n",
       "Question: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\n",
       "Answer: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. The answer is 8.\n",
       "\n",
       "Question: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\n",
       "Answer: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer is 9.\n",
       "\n",
       "Question: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
       "Answer: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. The answer is 8.\n",
       "\n",
       "\n",
       "Now, answer the following question\n",
       "Question: Four friends ordered four pizzas for a total of 64 dollars. If two of the pizzas cost 30 dollars, how much did each of the other two pizzas cost if they cost the same amount?\n",
       "     "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_prompt = prompt_fs_cot.invoke({\"example_qa\": example_qa, \"question\": question})\n",
    "display(Markdown(test_prompt.messages[1].content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Initialise a chat model using the Ollama LLM\n",
    "llm = ChatOllama(model=\"qwen2:1.5b\", temperature=0.0)\n",
    "\n",
    "# TODO: Create the LLM chain with the few-shot prompt template\n",
    "chain_fs_cot = prompt_fs_cot | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's denote the price of one pizza as \\(x\\). Since there are four pizzas and the total cost is $64, we can write:\n",
       "\n",
       "\\[4x = 64\\]\n",
       "\n",
       "Solving for \\(x\\), we find that each pizza costs $16. \n",
       "\n",
       "Now, let's consider two of these pizzas costing $30 in total. Let's denote the price of one of these pizzas as \\(y\\). So, the equation becomes:\n",
       "\n",
       "\\[2y + (x - y) = 30\\]\n",
       "\n",
       "Substituting \\(x\\) with $16 gives us:\n",
       "\n",
       "\\[2y + (16 - y) = 30\\]\n",
       "\\[y + 16 = 30\\]\n",
       "\\[y = 14\\]\n",
       "\n",
       "Therefore, each of the other two pizzas cost $14."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Run the chain with the question to get the string output\n",
    "response = chain_fs_cot.invoke({\"example_qa\": example_qa, \"question\": question})\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> üí° Note:\n",
    "> Through this demo, you should realise the benefit of using LangChain prompt templates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> üí¨ **Discussion:** \n",
    "> \n",
    "> Why the few-shot CoT prompting doesn't work in this example? Will the response change if we run multiple times? What if we change the temperature?\n",
    ">\n",
    "> üß† Critical Thinking: What are the limitations of CoT? How is the CoT technique incorporated in the reasoning models without requiring users to write CoT prompts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Limitations of Chain-of-Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "While Chain-of-Thought techniques provide significant improvements in reasoning capabilities, they also come with several important limitations:\n",
    "\n",
    "- **Reasoning Hallucinations**: \n",
    "  - LLMs may produce plausible-sounding but incorrect reasoning steps. These \"hallucinated\" steps can lead to wrong conclusions while appearing confident and logical.\n",
    "  - Example: When solving a complex physics problem, the model might introduce physically impossible intermediate calculations that seem reasonable but violate fundamental laws.\n",
    "\n",
    "- **Sensitivity to Prompt Wording**:\n",
    "  - CoT performance is highly dependent on how the prompt is phrased. Small changes in wording can lead to different reasoning paths and outcomes.\n",
    "  - Finding optimal prompts often requires extensive experimentation and may not generalize across different problem types.\n",
    "\n",
    "- **Computational Overhead**:\n",
    "  - CoT reasoning requires significantly more tokens than direct prompting, increasing:\n",
    "    - API costs when using commercial LLMs\n",
    "    - Latency in generating responses\n",
    "    - Computational resources needed\n",
    "\n",
    "- **Dependence on Example Quality in Few-shot CoT**\n",
    "  - The performance of few-shot CoT heavily depends on:\n",
    "    - The quality and relevance of chosen examples\n",
    "    - The similarity between examples and target problems\n",
    "    - The order in which examples are presented\n",
    "\n",
    "- **Limited Self-correction**:\n",
    "  - When a reasoning path leads to an error, LLMs often struggle to identify and correct the mistake, instead continuing with flawed reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Self Consistency Chain of Thought\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "An improvement on CoT prompting called \"Self Consistency\" is proposed by [Wang et al. (2022)](https://arxiv.org/abs/2203.11171). Self-consistency aims \"to replace the naive **greedy decoding** used in chain-of-thought prompting\". This approach **samples** multiple, diverse reasoning paths based on few-shot CoT, then select the most consistent answer among all reasoning paths. The evaluation shows it \"boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![SC CoT](imgs/L02_CoT_SC.png)\n",
    "\n",
    "> ü§ñ Implementation with AI\n",
    "> Based on the description of the Self-Consistency idea, think about how to implement it with the help of AI tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement the SC-CoT algorithm using local open-weight/source LLMs and LCEL (preferablly parallel prompting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An example of multi-path response (some parts are omitted due to the lengthy response):\n",
    "\n",
    "{'path_1': \"To find out how much each of the other two pizzas cost, let's break down what we know:\\n\\n1. Total cost for four pizzas: $64\\n2. Cost for two pizzas: $30\\n\\nLet's denote the price of one pizza as \\\\(x\\\\).\\n\\nSo, the equation representing the total cost would be:\\n\\\\[2x + 2x = 30\\\\]\\nSimplifying this gives us:\\n\\\\[4x = 30\\\\]\\n\\nTo find out how much each of the other two pizzas cost (\\\\(2x\\\\) since there are two), divide both sides by \\\\(2\\\\):\\n\\\\[x = \\\\frac{30}{2}\\\\]\\n\\\\[x = 15\\\\]\\n\\nSo, each of the other two pizzas costs $15.\", \n",
    "\n",
    "'path_2': \"To solve this problem, ... Therefore, each of the remaining two pizzas cost \\\\$17.\", \n",
    "\n",
    "'path_3': \"Let's break down the problem step-by-step.... Step 6: Present the final answer.\\n- Each of the other two pizzas costs $17.\\n\\nTherefore, the answer is $17.\", \n",
    "\n",
    "'path_4': ... Therefore, each of the other two pizzas cost $17.\", \n",
    "\n",
    "'path_5': 'To find out how much each of the other two pizzas cost ... Therefore, each of the two remaining pizzas cost $1.33.'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since multiple LLM responses must be sampled, the computational cost and response latency will be higher than the typical Chain of Thought (CoT) approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> üí¨ **Discussion:** \n",
    "> \n",
    "> üß† Critical Thinking: CoT technique can be embedded into a reasoning model by exposing a base model to CoT data in training/fine-tuning, which is called **train-time compute**. Is it reasonable to integrate SC-CoT into a model during training/fine-tuning? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> üí° Note:\n",
    "> When we sample multiple reasoing path in SC-CoT, the model will use longer time to think, i.e., compute for longer time during the test time. So, SC-CoT can be considered as a basic technique for **test-time compute**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation and Benchmarking\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![OpenAI o1 Benchmarks](./imgs/L01_O1_Benchmarks.png)\n",
    "\n",
    "Source: [OpenAI, Learning to Reason with LLMs](https://openai.com/index/learning-to-reason-with-llms/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![OpenAI o1 Benchmarks](./imgs/L01_DeepSeek_Benchmarks.png)\n",
    "\n",
    "Source: DeepSeek R1 paper ([DeepSeek-AI, 2024](https://arxiv.org/abs/2501.12948))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Evaluating reasoning capabilities of LLMs requires specialized benchmarks and metrics. Here we discuss some common approaches:\n",
    "\n",
    "**Common reasoning benchmarks:**\n",
    "- [GSM8K](https://huggingface.co/datasets/openai/gsm8k): Grade School Math problems requiring multi-step reasoning\n",
    "- [MATH 500](https://huggingface.co/datasets/HuggingFaceH4/MATH-500): A subset of 500 problems from the MATH benchmark that OpenAI created in their Let's Verify Step by Step paper.\n",
    "- [AIME 2024](https://huggingface.co/datasets/Maxwell-Jia/AIME_2024): This dataset contains problems from the American Invitational Mathematics Examination (AIME) 2024. AIME is a prestigious high school mathematics competition known for its challenging mathematical problems.\n",
    "- [GPQA Diamond](https://huggingface.co/datasets/Idavidrein/gpqa): The GPQA Diamond subset is a higher-quality, more challenging subset of the main GPQA dataset. It contains 198 questions for which both domain expert annotators got the correct answers, but which the majority of non-domain experts answered incorrectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Other benchmarks**\n",
    "- [MMLU](https://huggingface.co/datasets/Stevross/mmlu): Massive Multitask Language Understanding is a massive multitask test consisting of multiple-choice questions from various branches of knowledge. The test spans subjects in the humanities, social sciences, hard sciences, and other areas that are important for some people to learn. This covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability.\n",
    "- [CodeForces](https://huggingface.co/datasets/open-r1/codeforces): CodeForces is one of the most popular websites among competitive programmers, hosting regular contests where participants must solve challenging algorithmic optimization problems. The challenging nature of these problems makes them an interesting dataset to improve and test models‚Äô code reasoning capabilities.\n",
    "- [SWE-Bench](https://www.swebench.com/): A benchmark for evaluating large language models‚Äô (LLMs‚Äô) abilities to solve real-world software issues sourced from GitHub. The benchmark involves giving agents a code repository and issue description, and challenging them to generate a patch that resolves the problem described by the issue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Pass@k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In LLM evaluations, pass@k is a metric used to assess a model's ability to generate correct solutions (e.g., code, answers) by considering multiple attempts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> üí¨ **Discussion:** \n",
    "> \n",
    "> Try to learn from GenAI what \"pass@k\" means in LLM evaluations and use resources to find out whether GenAI is 100% correct.\n",
    ">\n",
    "> ü§ñ Reference prompt: *\"what does \"pass@k\" mean in LLM evaluations?\"*\n",
    ">\n",
    "> üß† Critical Thinking: Does the GenAI provide sources? Is the answer coherent? Is it reasonable when apply the definition to pass@1? Does it mentioned different ways to compute pass@k? Have you verified the responses from the GenAI you use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Put GenAI answers and your notes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The pass@k metric measures the probability, computed over a set of problems, that at least one of the top $k$ generated outputs for each problem contains the correct solution.\n",
    "\n",
    "For example, a Pass@1 of 30% and a Pass@10 of 60% would mean that the model has a 30% chance of solving the problem on the first try, but a 60% chance of finding a correct solution if allowed to generate 10 different attempts.\n",
    "\n",
    "- [Kulal et al. (2019)](https://proceedings.neurips.cc/paper/2019/file/7298332f04ac004a0ca44cc69ecf6f6b-Paper.pdf) evaluate functional correctness using the pass@k metric, where k samples are generated per problem, a problem is considered solved if any sample passes the unit tests, and the total fraction of problems solved is reported.\n",
    "- In practice, computing pass@k in this way can have high variance. For example, if we compute pass@1 from a single completion per problem, we can get significantly different values from repeated evaluations due to sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- [OpenAI (2021)](https://arxiv.org/abs/2107.03374) introduced an unbiased estimator that accounts for the total number of generated samples $n$, the number of correct samples $c$, and the desired $k$ value. To evaluate pass@k,\n",
    "  - generate $n \\geq k$ samples per problem/task\n",
    "  - count the number of correct samples $c \\leq n$\n",
    "  - calculate the unbiased estimator:\n",
    "\n",
    "$$\n",
    "\\text{pass@k} = \\mathbb{E}_{\\text{problems}} \\left[ 1 - \\frac{\\binom{n-c}{k}}{\\binom{n}{k}} \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- When calculate pass@1, [DeepSeek-AI, (2024)](https://arxiv.org/abs/2501.12948) uses a sampling temperature of 0.6 and a top-p value of 0.95 to generate $k$ responses (typically between 4 and 64, depending on the test set size) for each question and calcuates the pass@1 metric for their DeepSeek R1 model as:\n",
    "\n",
    "$$\n",
    "\\text{pass@k} = \\frac{1}{k} \\sum_{i=1}^k p_i\n",
    "$$\n",
    "\n",
    "where $p_i$ denotes the correctness of the $i$-th response.\n",
    "\n",
    "- For OpenAI o1 benchmark, they calculate pass@1 in the same way, as indicated by [OpenAI, (2024)]():\n",
    "> All models are given 5 tries to generate a candidate patch. We compute pass@1 by averaging the per-instance pass rates of all samples that generated a valid (i.e., non-empty) patch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example**\n",
    "Suppose one coding problem is evaluated with 100 samples ($n$=100), and 3 are correct ($c$=3):\n",
    "\n",
    "pass@1: 3 / 100 = 3% (probability a single random guess is correct).\n",
    "\n",
    "pass@3: $1 ‚àí \\frac{\\binom{100-3}{3}}{\\binom{100}{3}} \\approx 8.8\\%$\n",
    "\n",
    "pass@100: 100% (if at least one correct answer exists in 100 samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### cons@x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Consensus or majority vote, denoted as cons@x or maj@x, measures whether the most frequent answer (majority vote) among x generated responses is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Computational Cost: Generating many samples (e.g., n=100 n=100) is resource-intensive.\n",
    "- Domain-Specific: Works best for tasks with clear correctness criteria (e.g., code, math)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Reasoning techniques for LLMs have numerous practical applications across various domains:\n",
    "\n",
    "**Educational applications:**\n",
    "- Step-by-step math problem solving\n",
    "- Scientific reasoning and explanation\n",
    "- Tutoring with transparent reasoning\n",
    "\n",
    "**Business and finance:**\n",
    "- Financial analysis and planning\n",
    "- Risk assessment\n",
    "- Decision support systems\n",
    "\n",
    "**Healthcare:**\n",
    "- Diagnostic reasoning assistance\n",
    "- Treatment plan evaluation\n",
    "- Medical literature analysis\n",
    "\n",
    "**Software development:**\n",
    "- Code generation with explanation\n",
    "- Debugging assistance\n",
    "- Algorithm design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this notebook, we explored various reasoning techniques for LLMs,from Chain-of-Thought to Self Consistency to enhance problem-solving capabilities. Additionally, we cover evaluation metrics such as pass@k and cons@x, which are crucial for assessing the performance of LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Key Takeaways**\n",
    "1. **Reasoning Capabilities**: Modern LLMs demonstrate a range of reasoning abilities, including arithmetic, scientific, commonsense, logical, and visual reasoning.\n",
    "2. **Chain-of-Thought (CoT)**: CoT prompting significantly improves the reasoning performance of LLMs by breaking down complex problems into intermediate steps.\n",
    "3. **Self Consistency CoT**: Enhances CoT by sampling multiple reasoning paths and selecting the most consistent answer, further improving accuracy.\n",
    "4. **Evaluation Metrics**: Metrics like pass@k and cons@x are essential for evaluating the reasoning capabilities of LLMs.\n",
    "5. **Practical Applications**: Reasoning techniques have wide-ranging applications in education, business, healthcare, and software development, enhancing the utility and reliability of LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Reasoning techniques continue to evolve rapidly, enabling LLMs to tackle increasingly complex problems with greater reliability. By understanding and implementing these techniques, you can significantly enhance the capabilities of LLM applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bonus Scene\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the project diretory, run the `eight_puzzle.py` to play the eight puzzle game.\n",
    "\n",
    "```python\n",
    "python eight_puzzle.py\n",
    "```\n",
    "\n",
    "Can you solve the preset puzzle? How many steps it takes you to solve the puzzle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Put your notes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> üí¨ **Discussion:** \n",
    "> \n",
    "> Try to prompt the best reasoning LLMs (those you have access to) to solve the puzzle.\n",
    ">\n",
    "> ü§ñ Reference prompt: \n",
    "> ```text\n",
    "> please solve the 8-puzzle below, where 0 represents the blank tile. Please provide your thoughts about how to solve the problem and the solution step-by-step\n",
    "> [4, 1, 3],\n",
    "> [0, 8, 5],\n",
    "> [2, 7, 6]\n",
    "> ```\n",
    ">\n",
    "> üß† Critical Thinking: Can the smartest GenAI solve the puzzle only by the CoT reasoning? Can the GenAI models or you find the fewest steps to solve the puzzle only the the CoT reasoning? Check out the thougths generated by the reasoning model, do they mention any method to solve the puzzle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "DeepSeek thought for over 10 min! Unfortunately, it didn't solve the puzzle.\n",
    "\n",
    "![DeepSeek Thinking for 8-puzzle](./imgs/L01_BonusScene_DeepSeekThinking.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ReasoningAndAgents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
